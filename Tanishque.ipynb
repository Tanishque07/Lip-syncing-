{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE22UQtQ9YYi"
      },
      "source": [
        "# **Please use @justinjohn colab notebook which is better and actually works  https://colab.research.google.com/github/justinjohn0306/Wav2Lip/blob/master/Wav2Lip_simplified_v5.ipynb**\n",
        "\n",
        "# **There is no need for colab pro to use it.**\n",
        "[Discord](https://discord.gg/F5WzXC8vXJ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT9AQwdf8sJK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5taGmPcWV-"
      },
      "source": [
        "# **Get the code and models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgo-oaI3JU2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb057bc0-5384-41c5-b1e7-73afb7704f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "#@title <h1>Step1: Setup Wav2Lip</h1>\n",
        "#@markdown * Install dependency\n",
        "#@markdown * Download pretrained model\n",
        "!rm -rf /content/sample_data\n",
        "!mkdir /content/sample_data\n",
        "\n",
        "!git clone https://github.com/zabique/Wav2Lip\n",
        "\n",
        "#download the pretrained model\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'\n",
        "a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "\n",
        "# !pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip && pip install -r requirements.txt\n",
        "\n",
        "#download pretrained model for face detection\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\"\n",
        "\n",
        "!pip install -q youtube-dl\n",
        "!pip install ffmpeg-python\n",
        "!pip install librosa==0.9.1\n",
        "\n",
        "#this code for recording audio\n",
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzokJMO19IyY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgjaWJFs8B38"
      },
      "source": [
        "# **Quick guide**\n",
        "1. Create video file named input_video.mp4 - audio track removed\n",
        "2. Create audio file named input_audio.wav\n",
        "3. Both files have to be exact same length\n",
        "4. Target face in the input_video.mp4, must be \"detectable\" in ALL videoframes (So no black or blurry frames etc)\n",
        "5. Make sure u use correct file extensions\n",
        "6. wav2lip does not like very long and high res clips (1080p/30seconds max)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIQfY2Kswcb"
      },
      "source": [
        "# **Now lets try!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bDRYsfXTzXD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsphzJawLF-f"
      },
      "outputs": [],
      "source": [
        "# #@title 1.Upload input_video.mp4 & input_audio.wav files\n",
        "# %cd sample_data/\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnXNEDlPbWxo",
        "outputId": "9f11cda7-0a14-4101-9226-c75a2aff422d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/tanishque/'\n",
        "wav2lip_dir = '/content/Wav2Lip/'"
      ],
      "metadata": {
        "id": "zG6kNx_OiGXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/tanishque'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vjr_qs1cZ7k",
        "outputId": "56fdaa72-9df3-45b6-f7b3-ad2fd4555066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/tanishque\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ffmpeg\n",
        "\n",
        "# Input and Output directories\n",
        "input_dir = 'input'\n",
        "seg_dir = 'segmented'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(seg_dir, exist_ok=True)\n",
        "\n",
        "# Replace 'input_video.mp4' with the name of your uploaded video file in the 'input' folder\n",
        "input_file = os.path.join(input_dir, 'input_video.mp4')\n",
        "\n",
        "# Open the video file\n",
        "probe = ffmpeg.probe(input_file)\n",
        "video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')\n",
        "\n",
        "# Get the duration of the video in seconds\n",
        "duration = float(video_info['duration'])\n",
        "\n",
        "# Define the segment duration in seconds (20 seconds)\n",
        "segment_duration = 20\n",
        "\n",
        "# Calculate the number of segments\n",
        "num_segments = int(duration / segment_duration)\n",
        "\n",
        "# Lists to store segmented video and audio filenames\n",
        "segmented_video_files = []\n",
        "audio_files = []\n",
        "\n",
        "for i in range(num_segments):\n",
        "    start_time = i * segment_duration\n",
        "    end_time = (i + 1) * segment_duration\n",
        "    vid_output_file = os.path.join(seg_dir, f'vid_ns{i+1:04d}.mp4')\n",
        "    aud_output_file = os.path.join(seg_dir, f'aud_{i+1:04d}.wav')  # Change the extension to .wav\n",
        "\n",
        "    # Create the audio-less video\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_file, ss=start_time, t=segment_duration)\n",
        "        .output(vid_output_file, **{'c:v': 'copy', 'an': None})\n",
        "        .run()\n",
        "    )\n",
        "\n",
        "    # Extract audio and save as a separate WAV file\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_file, ss=start_time, t=segment_duration)\n",
        "        .output(aud_output_file, **{'c:a': 'pcm_s16le', 'vn': None})\n",
        "        .run()\n",
        "    )\n",
        "\n",
        "    # Append segmented video and audio filenames to lists\n",
        "    segmented_video_files.append(vid_output_file)\n",
        "    audio_files.append(aud_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzJotpizckud",
        "outputId": "f5393591-d52c-4c7a-cc92-ce811b6a9431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video successfully split into segments. Audio extracted in WAV format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR5utmDMcSZY"
      },
      "outputs": [],
      "source": [
        "# #@title 2.Create Wav2Lip video (using wav2lip_gan.pth) GAN\n",
        "# !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/Wav2Lip/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3l_jojjv8Qj",
        "outputId": "1d86ffe6-9ea6-4cb1-f020-c9769fdbce85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Wav2Lip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "# Input and Output directories\n",
        "base_dir = '/content/drive/My Drive/tanishque/'\n",
        "wav2lip_dir = '/content/Wav2Lip/'\n",
        "input_dir = os.path.join(base_dir, 'input')\n",
        "seg_dir = os.path.join(base_dir, 'segmented')\n",
        "processed_dir = os.path.join(base_dir, 'processed')\n",
        "\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# List of segmented video filenames\n",
        "segmented_video_files = [os.path.join(seg_dir, f'vid_ns{i+1:04d}.mp4') for i in range(num_segments)]\n",
        "\n",
        "# Specify the command template with the --outfile argument\n",
        "\n",
        "segmented_video_files = segmented_video_files[:3]\n",
        "print(segmented_video_files)\n",
        "# Iterate over segmented video files and run the command for each\n",
        "for i, segmented_video_file in enumerate(segmented_video_files):\n",
        "    input_audio = os.path.join(seg_dir, f'aud_{i+1:04d}.wav')\n",
        "    output_file = os.path.join(processed_dir, f'result_voice_{i+1:04d}.mp4')\n",
        "\n",
        "    # Build the command with the appropriate filenames using keyword arguments\n",
        "    command = \"python3 \\\"/content/Wav2Lip/inference.py\\\" --checkpoint_path \\\"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\\\" --face \" + \"\\\"\" + segmented_video_file + \"\\\" --audio \\\"\" + input_audio + \"\\\" --outfile \\\"\" + output_file + \"\\\"\"\n",
        "\n",
        "    # Run the command using subprocess\n",
        "    print(command)\n",
        "    subprocess.call(command, shell=platform.system() != 'Windows')\n",
        "\n",
        "print(\"Inference completed on all segmented videos.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "OKIhsHnjnnCE",
        "outputId": "9cd35976-590b-4da3-a7a0-c5cab0a81014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/My Drive/tanishque/segmented/vid_ns0001.mp4', '/content/drive/My Drive/tanishque/segmented/vid_ns0002.mp4', '/content/drive/My Drive/tanishque/segmented/vid_ns0003.mp4']\n",
            "python3 /content/Wav2Lip/inference.py --checkpoint_path \"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\" --face \"/content/drive/My Drive/tanishque/segmented/vid_ns0001.mp4\" --audio \"/content/drive/My Drive/tanishque/segmented/aud_0001.wav\" --outfile \"/content/drive/My Drive/tanishque/processed/result_voice_0001.mp4\"\n",
            "python3 /content/Wav2Lip/inference.py --checkpoint_path \"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\" --face \"/content/drive/My Drive/tanishque/segmented/vid_ns0002.mp4\" --audio \"/content/drive/My Drive/tanishque/segmented/aud_0002.wav\" --outfile \"/content/drive/My Drive/tanishque/processed/result_voice_0002.mp4\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-56f05c7acb4f>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Run the command using subprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Windows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inference completed on all segmented videos.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "command = \"python3 inference.py --checkpoint_path \\\"checkpoints/wav2lip_gan.pth\\\" --face \" + \"\\\"\" + segmented_video_file + \"\\\" --audio \\\"\" + input_audio + \"\\\" --outfile \\\"\" + output_file + \"\\\"\"\n",
        "print(command)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDupK3wjvqqP",
        "outputId": "fd541c8a-0aed-47ef-8d80-7fd9479336c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3 inference.py --checkpoint_path \"checkpoints/wav2lip_gan.pth\" --face \"/content/drive/My Drive/tanishque/segmented/vid_ns0003.mp4\" --audio \"/content/drive/My Drive/tanishque/segmented/aud_0003.wav\" --outfile \"/content/drive/My Drive/tanishque/processed/results/result_voice_0003.mp4\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"/content/Wav2Lip/inference.py\" --checkpoint_path \"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\" --face \"/content/drive/My Drive/tanishque/segmented/vid_ns0001.mp4\" --audio \"/content/drive/My Drive/tanishque/segmented/aud_0001.wav\" --outfile \"/content/drive/My Drive/tanishque/processed/result_voice_0001.mp4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyixwuhe8io5",
        "outputId": "f4731886-4dbd-49db-ab9a-dc9bef935e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 500\n",
            "/content/Wav2Lip/audio.py:100: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n",
            "(80, 1601)\n",
            "Length of mel chunks: 497\n",
            "  0% 0/4 [00:00<?, ?it/s]\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/32 [00:34<18:03, 34.96s/it]\u001b[A\n",
            "  6% 2/32 [00:37<07:58, 15.94s/it]\u001b[A\n",
            "  9% 3/32 [00:39<04:42,  9.75s/it]\u001b[A\n",
            " 12% 4/32 [00:42<03:11,  6.83s/it]\u001b[A\n",
            " 16% 5/32 [00:44<02:20,  5.22s/it]\u001b[A\n",
            " 19% 6/32 [00:47<01:51,  4.29s/it]\u001b[A\n",
            " 22% 7/32 [00:49<01:33,  3.75s/it]\u001b[A\n",
            " 25% 8/32 [00:52<01:19,  3.30s/it]\u001b[A\n",
            " 28% 9/32 [00:54<01:09,  3.01s/it]\u001b[A\n",
            " 31% 10/32 [00:56<01:01,  2.82s/it]\u001b[A\n",
            " 34% 11/32 [00:59<00:56,  2.68s/it]\u001b[A\n",
            " 38% 12/32 [01:01<00:53,  2.66s/it]\u001b[A\n",
            " 41% 13/32 [01:04<00:50,  2.65s/it]\u001b[A\n",
            " 44% 14/32 [01:06<00:46,  2.57s/it]\u001b[A\n",
            " 47% 15/32 [01:09<00:42,  2.50s/it]\u001b[A\n",
            " 50% 16/32 [01:11<00:40,  2.52s/it]\u001b[A\n",
            " 53% 17/32 [01:14<00:39,  2.63s/it]\u001b[A\n",
            " 56% 18/32 [01:17<00:38,  2.77s/it]\u001b[A\n",
            " 59% 19/32 [01:20<00:35,  2.73s/it]\u001b[A\n",
            " 62% 20/32 [01:23<00:32,  2.69s/it]\u001b[A\n",
            " 66% 21/32 [01:25<00:29,  2.66s/it]\u001b[A\n",
            " 69% 22/32 [01:28<00:26,  2.70s/it]\u001b[A\n",
            " 72% 23/32 [01:31<00:25,  2.84s/it]\u001b[A\n",
            " 75% 24/32 [01:34<00:22,  2.82s/it]\u001b[A\n",
            " 78% 25/32 [01:36<00:19,  2.74s/it]\u001b[A\n",
            " 81% 26/32 [01:39<00:16,  2.67s/it]\u001b[A\n",
            " 84% 27/32 [01:41<00:12,  2.57s/it]\u001b[A\n",
            " 88% 28/32 [01:44<00:10,  2.52s/it]\u001b[A\n",
            " 91% 29/32 [01:46<00:07,  2.55s/it]\u001b[A\n",
            " 94% 30/32 [01:49<00:04,  2.49s/it]\u001b[A\n",
            " 97% 31/32 [01:51<00:02,  2.47s/it]\u001b[A\n",
            "100% 32/32 [01:53<00:00,  3.56s/it]\n",
            "  0% 0/4 [01:55<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wav2Lip/inference.py\", line 280, in <module>\n",
            "    main()\n",
            "  File \"/content/Wav2Lip/inference.py\", line 249, in main\n",
            "    for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/content/Wav2Lip/inference.py\", line 113, in datagen\n",
            "    face_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n",
            "  File \"/content/Wav2Lip/inference.py\", line 92, in face_detect\n",
            "    raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
            "ValueError: Face not detected! Ensure the video contains a face in all the frames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "WxbzXZvLliiA"
      },
      "outputs": [],
      "source": [
        "#@title 3.Play result video -  50% scaling\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1kt-krsEbz5j",
        "outputId": "4b675100-7c3b-434f-d9a2-2aeeea1a3f43"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_d8aad7a1-02ca-4045-ab68-bf606791fa82\", \"result_voice.mp4\", 3170343)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title 4.Download Result.mp4 to your computer\n",
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fT8njpBCJ7gD"
      },
      "outputs": [],
      "source": [
        "#@title 5. Delete old uploaded samples & result files, so you can start over again.\n",
        "%rm /content/sample_data/*\n",
        "%rm /content/Wav2Lip/results/*\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\nDone! now press X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgMkHOFqT2fK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "2OhT0w2uT4rQ"
      },
      "outputs": [],
      "source": [
        "#@title 1-4. Batch processing - Upload -> process -> download -> play result\n",
        "%cd sample_data/\n",
        "%rm input_audio.wav\n",
        "%rm input_video.mp4\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%cd ..\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\"\n",
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4')\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9NvwrJ3vRcs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7zgfrQqbKom"
      },
      "source": [
        "# **Variations to try**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xw0xFtZ2bsx8"
      },
      "outputs": [],
      "source": [
        "#@title 2.Use resize_factor to reduce the video resolution, as there is a change you might get better results for lower resolution videos. Why? Because the model was trained on low resolution faces.\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\" --resize_factor 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "45XW4SZAzIz5"
      },
      "outputs": [],
      "source": [
        "#@title 3.Use more padding to include the chin region (u can manually edit pads dimensions viewing and changing the code)\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\" --pads 0 20 0 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "X1Z0zRdZR5BZ"
      },
      "outputs": [],
      "source": [
        "#@title 4.Play result video -  50% scaling\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfXFOpAmR_dh"
      },
      "outputs": [],
      "source": [
        "#@title 5.Download Result.mp4 to your computer\n",
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}